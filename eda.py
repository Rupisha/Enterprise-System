# -*- coding: utf-8 -*-
"""EDA Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RHlGivCF_lH0J0Hn8MwJ8J_pkRpopeGE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import string
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

df=pd.read_csv("D:\M.Tech\SEM1\EDA\AmzReviews.csv")
df.head(10)

print(df.shape)
print(df.isnull().values.any())
df.dropna(axis = 0, inplace = True)
print(df.shape)

df.drop_duplicates(subset =['Score','Text'],keep='first', inplace=True)
print(df.shape)
df.head(10)

plt.figure(figsize=(10,10))
ax = sns.countplot(x = df['Score'], data=df, order = df['Score'].value_counts().index)
for p, label in zip(ax.patches, df['Score'].value_counts()):
    ax.annotate(label, (p.get_x()+0.25, p.get_height()+0.5))

df.groupby('ProductId').count()

"""###  filter those products which have atleast 400 reviews"""

df_products = df.groupby('ProductId').filter(lambda x: len(x) >= 400)
df_product_groups = df_products.groupby('ProductId')

print(len(df_products))
print(len(df_product_groups))

plt.figure(figsize=(20,20))
sns.countplot(y="ProductId",  hue="Score", data=df_products);

"""filter by User's who have more than 100 reviews"""

df.groupby('UserId').count()

df_users = df.groupby('UserId').filter(lambda x: len(x) >= 10)
df_userGroup = df_users.groupby('UserId')
print(len(df_userGroup))

df_users = df.groupby('UserId').filter(lambda x: len(x) >= 5)
df_userGroup = df_users.groupby('UserId')
print(len(df_userGroup))

df_users = df.groupby('UserId').filter(lambda x: len(x) >= 2)
df_userGroup = df_users.groupby('UserId')
print(len(df_userGroup))

df_users = df.groupby('UserId').filter(lambda x: len(x) >= 50)
df_userGroup = df_users.groupby('UserId')
print(len(df_userGroup))

df_users = df.groupby('UserId').filter(lambda x: len(x) >= 100)
df_userGroup = df_users.groupby('UserId')
print("Number of Users:"+ str(len(df_userGroup)))
df_products = df_users.groupby('ProductId')
print("Number of products:"+ str(len(df_products)))

plt.figure(figsize=(20,40))
sns.countplot(y="UserId", hue="Score", data=df_users )

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

def remove_Stopwords(text ):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize( text.lower() )
    sentence = [w for w in words if not w in stop_words]
    return " ".join(sentence)

def lemmatize_text(text):
    wordlist=[]
    lemmatizer = WordNetLemmatizer()
    sentences=sent_tokenize(text)
    for sentence in sentences:
        words=word_tokenize(sentence)
        for word in words:
            wordlist.append(lemmatizer.lemmatize(word))
    return ' '.join(wordlist)

def clean_text(text ):
    delete_dict = {sp_character: '' for sp_character in string.punctuation}
    delete_dict[' '] = ' '
    table = str.maketrans(delete_dict)
    text1 = text.translate(table)
    #print('cleaned:'+text1)
    textArr= text1.split()
    text2 = ' '.join([w for w in textArr])

    return text2.lower()

mask = (df["Score"] == 1) | (df["Score"] == 2)
df_rating12 = df[mask]
mask = (df["Score"]==4) | (df["Score"]==5) | (df["Score"]==3)
df_rating345 = df[mask]

print(len(df_rating12))
print(len(df_rating345))

import nltk
nltk.download('wordnet')

df_rating12['Text'] = df_rating12['Text'].apply(clean_text)
df_rating12['Text'] = df_rating12['Text'].apply(remove_Stopwords)
df_rating12['Text'] = df_rating12['Text'].apply(lemmatize_text)


df_rating345['Text'] = df_rating345['Text'].apply(clean_text)
df_rating345['Text'] = df_rating345['Text'].apply(remove_Stopwords)
df_rating345['Text'] = df_rating345['Text'].apply(lemmatize_text)

df_rating12['Num_words_text'] = df_rating12['Text'].apply(lambda x:len(str(x).split()))
df_rating345['Num_words_text'] = df_rating345['Text'].apply(lambda x:len(str(x).split()))

df_rating12['Num_words_text'].describe()

df_rating345['Num_words_text'].describe()

wordcloud = WordCloud(background_color="white",width=1600, height=800).generate(' '.join(df_rating12['Summary'].tolist()))
plt.figure( figsize=(20,10), facecolor='k')
plt.imshow(wordcloud)

wordcloud = WordCloud(background_color="white",width=1600, height=800).generate(' '.join(df_rating345['Summary'].tolist()))
plt.figure( figsize=(20,10), facecolor='k')
plt.imshow(wordcloud)
plt.axis("off")

df = df[df['Score'] != 3]
X = df['Text']
y_dict = {1:0, 2:0, 4:1, 5:1}
y = df['Score'].map(y_dict)

def text_fit(X, y, model,clf_model,coef_show=1):

    X_c = model.fit_transform(X)
    print('# features: {}'.format(X_c.shape[1]))
    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0)
    print('# train records: {}'.format(X_train.shape[0]))
    print('# test records: {}'.format(X_test.shape[0]))
    clf = clf_model.fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    print ('Model Accuracy: {}'.format(acc))

    if coef_show == 1:
        w = model.get_feature_names()
        coef = clf.coef_.tolist()[0]
        coeff_df = pd.DataFrame({'Word' : w, 'Coefficient' : coef})
        coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])
        print('')
        print('-Top 10 positive-')
        print(coeff_df.head(10).to_string(index=False))
        print('')
        print('-Top 10 negative-')
        print(coeff_df.tail(10).to_string(index=False))
        c_d_h_r = coeff_df.head(10)
        c_d_t_r = coeff_df.tail(10)
        return c_d_h_r,c_d_t_r

tfidf = TfidfVectorizer(stop_words = 'english')
c_d_h_r, c_d_t_r = text_fit(X, y, tfidf, LogisticRegression())

plt.figure(figsize=(20,20))
sns.barplot(c_d_h_r['Word'], c_d_h_r['Coefficient'])

plt.figure(figsize=(20,20))
sns.barplot(c_d_t_r['Word'], c_d_t_r['Coefficient'])